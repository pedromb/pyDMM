import joblib
from gibbsdmm import GibbsSamplingDMM

class GSDMM:
    
    """Implementation of Dirichlet Multinomial Mixture model using collapsed Gibbs sampling.

    Attributes
    ----------
    documents : :list: of :list: `str`
        List of texts to make inference. 
        Each element should be the tokenized/pre_processed text
    alpha : `float`
        Hyperparameter alpha
    beta : `float`
        Hyperparameter beta
    ntopics : `int`
        Number of topics
    niters: `int`
        Number of Gibbs sampling iterations
    top_words : `int`
        Number of probable words per topic
    verbose : `bool`
        Wheter or not to print eventual logs
    topic_assignments : :list: `int`
        List of topics assigned to each document
    """

    def __init__(self, documents, alpha=0.1, beta=0.01, ntopics=20, niters=2000, top_words=20, verbose=True):
        self.obj = GibbsSamplingDMM(documents, alpha, beta, ntopics, niters, top_words, verbose)
        self.obj.analyse_corpus()
        self.obj.initialize_topic_assignments()

    def fit(self):
        """ Run Gibbs sampling to create the model
        """
        self.obj.fit()
    
    def show_topics(self, formatted=True, ntopics=None, top_words=20):
        """ Show topics generated by running Gibbs sampling
        Use it after calling fit.
        
        Args
        ----------
        formatted : `bool`
            If its true it will return a formatted string with the topics and the words
            that are part of the topic with their respective probability
        ntopics : `int`
            Number of topics to be returned

        Returns
        ----------
        topics : `string` or `dict`
            If formatted is True it will return a formatted list of ntopics
            If formatted is False it returns dictionary, where the topic is the key and the value is a
                dict where the key is a word and the value is the word probability for that topic.
        """
        topic_word_count = self.obj.topic_word_count
        sum_topic_word_count = self.obj.sum_topic_word_count
        id_to_word_vocabulary = self.obj.id_to_word_vocabulary
        vocabulary_size = self.obj.vocabulary_size
        beta = self.obj.beta
        beta_sum = self.obj.beta_sum
        total_topics = self.obj.topics_convergence[-1]
        if ntopics is None:
            ntopics = total_topics
        final_text = ""
        res = {}
        t_index = 0
        while len(res) < ntopics:
            if sum_topic_word_count[t_index] > 0:
                res[t_index] = {}
                topic_text = "Topic {}:".format(t_index)
                word_count = {w_index:topic_word_count[t_index][w_index] for w_index in range(vocabulary_size)}
                word_count = sorted(word_count.items(), key=lambda x: x[1], reverse=True)
                count = 0
                for k, v in word_count:
                    if count < top_words:
                        prob = (topic_word_count[t_index][k] + beta)/(sum_topic_word_count[t_index] + beta_sum)
                        prob = round(prob*1000000)/1000000
                        topic_text += " {}({})".format(id_to_word_vocabulary[k], prob)
                        res[t_index][id_to_word_vocabulary[k]] = prob
                        count += 1
                    else:
                        topic_text += "\n\n"
                        final_text += topic_text
                        break
            t_index += 1
        if formatted:
            return final_text
        else:
            return res
                
    def predict(self, unseen_docs, niters=100, probs=False):
        """ Predict the topic for an unseen text
        Use it after calling fit.
        
        Args
        ----------
        unseen_docs : :list: of :list: `str`
            List of unseen documents, each unseen document is a list of tokens for a text
            The text should already be pre processed and tokenized
        niters : `int`
            Number of times to run Gibbs sampling before predicting the topic
        probs : `bool`
            Wheter or not to return the probability for each topic

        Returns
        ----------
        topics_assignments : :list: `int` or :list: `dict`
            If probs is False it returns a list with the topic assigned to each input document
            If probs is True it returns a list of dicts, each dict has the topic as key
                and the probability of that topic for the document as value
        """
        return self.obj.predict(unseen_docs, niters, probs)
            
    def save(self, filename):
        """ Saves the trained model
        Use it after calling fit.
        
        Args
        ----------
        filename : `str`
           Path where to save the file
        """
        if filename is None:
            print("Please provide a path to save the file")
        else:
            joblib.dump(self, filename)
    
    @staticmethod
    def load(filename):
        """ Loads a saved model
        
        Args
        ----------
        filename : `str`
           Path to the file to be loaded

        Returns
        ----------
        model : `GibbsSamplingDMM`
            Returns the model saved on the file
        """
        if filename is None:
            print("Please provide a path to load the model from")
        else:
            return joblib.load(filename)